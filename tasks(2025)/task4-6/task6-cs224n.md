# Task 6 LLM 入门

## 学习目的

祝贺你坚持到这里。你已经完成了前期的系统学习：在 Task 4 中，你实现了反向传播算法；在 Task 5 中，你使用模块化的方法构建了现代卷积神经网络的架构。你不再仅是调用 API 的使用者，而是真正理解神经网络内部运作机制的开发者。

现在，欢迎来到 NLP 学习阶段的开始，也是基础学习阶段的重要总结。

在本阶段的学习中，我们将从相对基础的文本分类、序列标注任务扩展到更广阔、更深入的自然语言理解（Understanding）与生成（Generation）领域。你将学习语言是如何被赋予数学形式的，机器是如何解析复杂句法结构的，以及如何实现不同语言之间的翻译。

本次任务将是你从理解经典模型到接触前沿研究的过渡。完成它，意味着你将具备初步阅读 NLP 顶级会议论文、理解并实现复杂语言系统的核心能力。

## 学习内容

本阶段你将接触到一系列定义了现代 NLP 研究方向的重要模型与方法：

* 词的向量表示（Word Embeddings）：学习 `word2vec` 的原理，理解计算机如何通过向量运算捕捉词语间的语义关系。
* 句法结构分析（Syntactic Parsing）：学习并实现一个基于神经网络的依存句法分析器（Dependency Parser），让模型理解句子的结构。
* 序列到序列模型（Sequence-to-Sequence Models）：构建一个带注意力机制（Attention）的神经机器翻译（Neural Machine Translation, NMT）系统，体验模型在翻译时的注意力分配。
* Transformer 架构（Transformer Architecture）：深入理解驱动了当今大语言模型发展的核心——自注意力机制（Self-Attention），并亲手实现其关键组件。

## 学习要求

与前几次作业相比，本阶段任务对数学基础、理论深度和代码实现精度的要求达到了最高水平。

1. 深入理论学习：CS224n 以其理论深度著称。你需要花费大量时间消化课程笔记，彻底理解反向传播在 RNN（BPTT）、Attention 和 Transformer 中的推导细节。
2. 代码精确实现：本次作业中的很多模块（如 NMT 的 `forward` 和 `backward`）对代码的精确度要求极高，一个微小的索引错误或维度不匹配都可能导致整个系统出错。
3. 系统级整合与调试：你需要将编码器、解码器、注意力模块等多个复杂组件准确地组合起来。调试这样一个庞大的系统极具挑战，你需要更有耐心，并学会如何设计单元测试来验证每个模块的正确性。

## 作业

关于本次的任务，你需要完成以下内容——

* 作业内容中提到的所有文档（不出意外的话是 2 份）
* 完成作业一
* 从作业二、三中选择一项完成

### 作业 1

完成 [CS224n 四大编程作业](https://web.stanford.edu/class/cs224n/)

本次考核由四个模块构成，它们将引领你逐步掌握 NLP 领域的核心技术。

> 建议选择 LLM 路线的同学先完成 cs224n 的前三个 assignment，然后再去完成 cs231n（毕竟都写了前两个了）。

#### 词向量的探索与应用（Assignment 1）

在这一部分，你将深入学习词语在向量空间中的表示方法。你需要：

* 理解 `word2vec` 模型（CBOW 和 Skip-Gram）背后的核心思想。
* 使用 `gensim` 等工具进行实践，并完成基于词向量的语义类比测试（如「king - man + woman ≈ queen」），直观感受词向量的效果。

#### 神经网络与依存句法分析（Assignment 2）

在理解了词的表示后，你将开始让机器理解句子的结构。你需要：

* 实现一个基于神经网络的依存句法分析器。
* 深刻理解模型是如何通过分析句子中词语之间的修饰与被修饰关系，来形成对整个句子结构的语法树表示的。

#### 神经机器翻译（Assignment 3）

这是最具挑战性的任务之一，你将构建一个能将一种语言翻译成另一种语言的系统。你需要：

* 实现一个带有注意力机制（Attention）的编码器-解码器（Encoder-Decoder）模型。
* 在训练过程中，你将能观察到模型在生成每一个目标语言单词时，它的注意力是如何在源语言句子的不同部分上动态分配的。

#### 探索 Transformer（Assignment 4）

在体验了 RNN 处理序列的模式后，你将接触到当今 AI 领域的主流方法——Transformer。你需要：

* 学习并亲手实现自注意力机制（Self-Attention）和多头注意力（Multi-Head Attention）的核心计算过程。
* 通过构建一个简化的 Transformer 模型，你将彻底理解它为何能并行处理序列，并成为 GPT 等所有大语言模型的基础架构。

#### 重要提示

* 关于最终项目（Final Project）：为了让大家能将精力完全集中在对核心知识的编码实现上，本次考核无需完成 CS224n 的最终项目（Final Project）
* 关于翻译与学习资料：在学习过程中，如果遇到困难，可以参考：
  * [ShaddockNH3/CS224N-Nyan-Book](https://github.com/ShaddockNH3/CS224N-Nyan-Book)
  * 这是本人的学习仓库，你可以在这里找到课程 PPT 的翻译、论文精读笔记等学习资料，尤其是环境配置（cs224n 的环境配置没有 cs231n 配置那么方便）。

#### 作业 1 - 作业要求

在完成作业后，你需要写一份**文档**，内容包括但不限于：

1. 各个模型的实现细节与数学原理
2. 各个模型词向量表示、句法分析、机器翻译、Transformer 的性能比较与分析
3. 你在实现过程中遇到的挑战与解决方案

#### 作业参考资料

0. [cs224n](https://www.bilibili.com/video/BV1vQMBz6EvP/?spm_id_from=333.337.search-card.all.click&vd_source=0272bb7dd0d8d9302c55fc082442b9e3)，能够理解 ppt 和论文的可以不用看视频
1. [跟李沐学AI 词向量（word2vec）【动手学深度学习v2】](https://www.bilibili.com/video/BV1sY4y1572C/)
2. [跟李沐学AI 注意力机制【动手学深度学习v2】](https://www.bilibili.com/video/BV1ui4y1j783/)
3. [跟李沐学AI Transformer论文逐段精读【论文精读】](https://www.bilibili.com/video/BV1pu411o7BE/)
4. [【Transformer 其实是个简单到令人困惑的模型【白话DeepSeek-06】】](https://www.bilibili.com/video/BV1C3dqYxE3q/)
5. [台大李宏毅老师 机器学习2021（Self-Attention和Transformer部分）](https://www.bilibili.com/video/BV1JA411X76s?p=65)

### 作业 2：情感分类器与注意力可视化

在 Task 5 中，你从零开始训练了一个卷积神经网络。你可能发现：为了提高一点点准确率，需要调参很久，而且训练非常耗时。

在本次作业中，你将搭建一个简单的情感分类器，并学习如何使用预训练的词向量 GloVe 来提升模型效果。最后，你还将实现注意力机制的可视化，探索模型的可解释性。

> 本作业禁止使用 Transformer 库，只能使用 PyTorch 等基础库来实现模型。

#### 情感分类器

你将先从零开始实现一个简单的情感分类器。

爬取数据：

1. 你可以使用 IMDb 影评数据集（[下载链接](https://ai.stanford.edu/~amaas/data/sentiment/)），它包含 50,000 条影评，分为正面和负面两类。
2. 或者你可以自行使用其他数据集，亦或自行收集数据后进行标注。
3. 如果你想自行收集数据后标注，可以使用 [Label Studio](https://labelstud.io/) 这类工具来辅助标注工作。
4. 爬取数据后，你需要将数据按照训练集、验证集和测试集进行划分，常见的比例是 80% 训练集，10% 验证集，10% 测试集。

数据预处理：

1. 你需要对文本数据进行预处理，包括分词、去除停用词、构建词汇表等。
2. 中文可以使用 `jieba` 进行分词，英文直接按空格分词即可。
3. 遍历训练集，统计所有出现的词，构建 `word2idx` 字典和 `idx2word` 列表。
4. 别忘了添加特殊标记，如 `<PAD>`（用于填充）、`<UNK>`（用于未登录词）等。

模型构建：

1. 使用 nn.Embedding 层将词索引转换为词向量。
2. Encoder 层可以从最简单的 RNN 开始实现。
3. 分类头是一个简单的全连接层 nn.Linear，将 RNN 的最终状态映射到分类空间

进阶一些：

1. 情感不只有正面和负面，你可以尝试实现多分类情感分类器（如 5 分类）。
2. 你可以尝试使用不同的网络结构，如 LSTM、 GRU，甚至是简单的全连接网络，比较它们在情感分类任务上的表现。
3. 你可以尝试使用不同的优化器（如 Adam、RMSprop）和学习率调度器，观察它们对模型训练的影响。

提示：

1. 句子的长度不一，你需要对输入数据进行填充或截断，以确保每个批次中的句子长度一致。

#### GloVe

在你实现完情感分类器后，你可能会发现模型的效果并不理想。收敛速度慢，而且对于没见过的词表现很差。

在现实世界中，我们很少从零开始训练 Embedding，因为像 Google、Stanford 这样的巨头已经用海量的文本（比如整个维基百科）训练好了极其强大的词向量。

现在你暂且放下刚刚实现好的情感分类器，学习如何使用预训练的词向量 GloVe 来提升模型效果。

你将手动加载 GloVe 词向量，替换原有的随机初始化 Embedding 层，并对比冻结参数和微调参数两种方式的效果差异。

下载与解析：

1. 下载 glove.6B.50d.zip（[下载链接](https://nlp.stanford.edu/projects/glove/)），解压后得到 glove.6B.50d.txt 文件。
2. 编写一个函数，读取这个 txt 文件，将其解析为一个字典，键是词语，值是对应的 50 维向量。

> 提示：调试代码时，可以只读取前 10,000 行 GloVe 向量，跑通了再加载全部。

构建权重矩阵：

1. 创建一个形状为 (vocab_size, embedding_dim) 的权重矩阵。
2. 遍历此前构建的词表。如果这个词在 GloVe 词典中，使用对应的向量填充权重矩阵；否则，使用随机初始化的向量。

加载到模型：

1. 使用 `nn.Embedding.from_pretrained(weight_matrix)` 方法，加载你构建的矩阵
2. 设置 `freeze=True` 来冻结词向量参数
3. 设置 `freeze=False` 来允许微调。

对比分析：

对比冻结参数和微调参数前 5 个 epoch 的训练和验证准确率，分析两者的优缺点。理论上，使用了 Glove 的模型起步会非常快。

#### 词向量可视化

语言是人类智慧的结晶。机器真的能理解我们在说什么吗？

还是它只是在做概率统计游戏？

为了探索这个问题，我们可以通过降维算法将高维向量画在二维空间平面上。

提取权重：

从你训练好的模型（或者直接从 GloVe）中，提取 Embedding 层的权重矩阵。

降维度：

1. 使用 `sklearn.manifold.TSNE` 或 `PCA`，将高维词向量降到二维。
2. 注意，不要绘制几万个词，随机选取 200 个词，或者手动选取几组有趣的词（如「king」、「queen」、「man」、「woman」等）。

绘图与分析：

1. 使用 `matplotlib`，将降维后的词向量绘制在二维平面上，并在点旁边标注单词。
2. 观察并分析词语之间的空间关系，看看是否能发现一些有趣的语义关系。

Attention 可视化（可选进阶）：

如果你想深入研究，可以使用 Attention 机制绘制注意力热力图，展示模型在处理句子时关注的词语。

#### 思考

完成上面的所有任务之后，你可以思考一下：

1. 预训练词向量真的能让模型更快地收敛吗？为什么？
2. 冻结参数和微调参数两种方式，哪个更适合你的情感分类器？为什么？
3. 通过词向量可视化，你能发现哪些有趣的语义关系？
4. 语义关系之中，例如，「man」和「programmer」可能距离更近，而「woman」和「programmer」距离更远，这反映了现实世界中的性别偏见。你觉得这种偏见是如何产生的？我们应该如何应对它？
5. 你觉得机器真的理解了语言吗？还是它只是通过统计学方法捕捉了词语之间的关系？

#### 作业 2 - 作业要求

在完成作业后，你需要写一份**文档**，内容包括但不限于：

1. 对比分析图
2. 代码实现细节
3. 词向量可视化图
4. （可选）展示至少 3 个句子的 Attention 热力图，并且分析模型的可解释性。如果模型判断错了，通过热力图能够看出原因吗？

以及你遇到的问题，等等。

### 作业 3

详见 CV 方向的作业 3。
