# Task 6 LLM 入门

## 学习内容

1. 词向量与语言表示：从 Word2Vec 到 GloVe，掌握将人类语言转化为数学向量的技术。
2. 序列模型 (RNN/LSTM)：学习处理变长文本数据，理解门控机制如何捕捉上下文依赖。
3. Transformer 与注意力：深入剖析 Self-Attention 与 Encoder-Decoder 架构，掌握 LLM 的基石。
4. 情感分析与可视化：实战搭建分类模型，并通过 t-SNE 与热力图探索语义关系与可解释性。

## 作业

关于本次的任务，你需要完成以下内容——

* 作业内容中提到的所有文档（不出意外的话是 2 份）
* 完成作业一
* 从作业二、三中选择一项完成

### 作业 1

完成 [CS224n 四大编程作业](https://web.stanford.edu/class/cs224n/)

> 建议选择 LLM 路线的同学先完成 cs224n 的前三个 assignment，然后再去完成 cs231n（毕竟都写了前两个了）。

#### 提示

* 关于最终项目（Final Project）：为了让大家能将精力完全集中在对核心知识的编码实现上，本次考核无需完成 CS224n 的最终项目（Final Project）
* 关于翻译与学习资料：在学习过程中，如果遇到困难，可以参考：
  * [ShaddockNH3/CS224N-Nyan-Book](https://github.com/ShaddockNH3/CS224N-Nyan-Book)
  * 这是本人的学习仓库，你可以在这里找到课程 PPT 的翻译、论文精读笔记等学习资料，尤其是环境配置（cs224n 的环境配置没有 cs231n 配置那么方便）。

#### 作业 1 - 作业要求

在完成作业后，你需要写一份**文档**，内容包括但不限于：

1. 各个模型的实现细节与数学原理
2. 各个模型词向量表示、句法分析、机器翻译、Transformer 的性能比较与分析
3. 你在实现过程中遇到的挑战与解决方案

#### 作业参考资料

0. [cs224n](https://www.bilibili.com/video/BV1vQMBz6EvP/?spm_id_from=333.337.search-card.all.click&vd_source=0272bb7dd0d8d9302c55fc082442b9e3)，能够理解 ppt 和论文的可以不用看视频
1. [跟李沐学AI 词向量（word2vec）【动手学深度学习v2】](https://www.bilibili.com/video/BV1sY4y1572C/)
2. [跟李沐学AI 注意力机制【动手学深度学习v2】](https://www.bilibili.com/video/BV1ui4y1j783/)
3. [跟李沐学AI Transformer论文逐段精读【论文精读】](https://www.bilibili.com/video/BV1pu411o7BE/)
4. [【Transformer 其实是个简单到令人困惑的模型【白话DeepSeek-06】】](https://www.bilibili.com/video/BV1C3dqYxE3q/)
5. [台大李宏毅老师 机器学习2021（Self-Attention和Transformer部分）](https://www.bilibili.com/video/BV1JA411X76s?p=65)

### 作业 2：情感分类器与注意力可视化

在 Task 5 中，你从零开始训练了一个卷积神经网络。你可能发现：为了提高一点点准确率，需要调参很久，而且训练非常耗时。

在本次作业中，你将搭建一个简单的情感分类器，并学习如何使用预训练的词向量 GloVe 来提升模型效果。最后，你还将实现注意力机制的可视化，探索模型的可解释性。

> 本作业禁止使用 Transformer 库，只能使用 PyTorch 等基础库来实现模型。

#### 情感分类器

你将先从零开始实现一个简单的情感分类器。

爬取数据：

1. 你可以使用 IMDb 影评数据集（[下载链接](https://ai.stanford.edu/~amaas/data/sentiment/)），它包含 50,000 条影评，分为正面和负面两类。
2. 或者你可以自行使用其他数据集，亦或自行收集数据后进行标注。
3. 如果你想自行收集数据后标注，可以使用 [Label Studio](https://labelstud.io/) 这类工具来辅助标注工作。
4. 爬取数据后，你需要将数据按照训练集、验证集和测试集进行划分，常见的比例是 80% 训练集，10% 验证集，10% 测试集。

数据预处理：

1. 你需要对文本数据进行预处理，包括分词、去除停用词、构建词汇表等。
2. 中文可以使用 `jieba` 进行分词，英文直接按空格分词即可。
3. 遍历训练集，统计所有出现的词，构建 `word2idx` 字典和 `idx2word` 列表。
4. 别忘了添加特殊标记，如 `<PAD>`（用于填充）、`<UNK>`（用于未登录词）等。

模型构建：

1. 使用 nn.Embedding 层将词索引转换为词向量。
2. Encoder 层可以从最简单的 RNN 开始实现。
3. 分类头是一个简单的全连接层 nn.Linear，将 RNN 的最终状态映射到分类空间

进阶一些：

1. 情感不只有正面和负面，你可以尝试实现多分类情感分类器（如 5 分类）。
2. 你可以尝试使用不同的网络结构，如 LSTM、 GRU，甚至是简单的全连接网络，比较它们在情感分类任务上的表现。
3. 你可以尝试使用不同的优化器（如 Adam、RMSprop）和学习率调度器，观察它们对模型训练的影响。

提示：

1. 句子的长度不一，你需要对输入数据进行填充或截断，以确保每个批次中的句子长度一致。

#### GloVe

在你实现完情感分类器后，你可能会发现模型的效果并不理想。收敛速度慢，而且对于没见过的词表现很差。

在现实世界中，我们很少从零开始训练 Embedding，因为像 Google、Stanford 这样的巨头已经用海量的文本（比如整个维基百科）训练好了极其强大的词向量。

现在你暂且放下刚刚实现好的情感分类器，学习如何使用预训练的词向量 GloVe 来提升模型效果。

你将手动加载 GloVe 词向量，替换原有的随机初始化 Embedding 层，并对比冻结参数和微调参数两种方式的效果差异。

下载与解析：

1. 下载 glove.6B.50d.zip（[下载链接](https://nlp.stanford.edu/projects/glove/)），解压后得到 glove.6B.50d.txt 文件。
2. 编写一个函数，读取这个 txt 文件，将其解析为一个字典，键是词语，值是对应的 50 维向量。

> 提示：调试代码时，可以只读取前 10,000 行 GloVe 向量，跑通了再加载全部。

构建权重矩阵：

1. 创建一个形状为 (vocab_size, embedding_dim) 的权重矩阵。
2. 遍历此前构建的词表。如果这个词在 GloVe 词典中，使用对应的向量填充权重矩阵；否则，使用随机初始化的向量。

加载到模型：

1. 使用 `nn.Embedding.from_pretrained(weight_matrix)` 方法，加载你构建的矩阵
2. 设置 `freeze=True` 来冻结词向量参数
3. 设置 `freeze=False` 来允许微调。

对比分析：

对比冻结参数和微调参数前 5 个 epoch 的训练和验证准确率，分析两者的优缺点。理论上，使用了 Glove 的模型起步会非常快。

#### 词向量可视化

语言是人类智慧的结晶。机器真的能理解我们在说什么吗？

还是它只是在做概率统计游戏？

为了探索这个问题，我们可以通过降维算法将高维向量画在二维空间平面上。

提取权重：

从你训练好的模型（或者直接从 GloVe）中，提取 Embedding 层的权重矩阵。

降维度：

1. 使用 `sklearn.manifold.TSNE` 或 `PCA`，将高维词向量降到二维。
2. 注意，不要绘制几万个词，随机选取 200 个词，或者手动选取几组有趣的词（如「king」、「queen」、「man」、「woman」等）。

绘图与分析：

1. 使用 `matplotlib`，将降维后的词向量绘制在二维平面上，并在点旁边标注单词。
2. 观察并分析词语之间的空间关系，看看是否能发现一些有趣的语义关系。

Attention 可视化（可选进阶）：

如果你想深入研究，可以使用 Attention 机制绘制注意力热力图，展示模型在处理句子时关注的词语。

#### 思考

完成上面的所有任务之后，你可以思考一下：

1. 预训练词向量真的能让模型更快地收敛吗？为什么？
2. 冻结参数和微调参数两种方式，哪个更适合你的情感分类器？为什么？
3. 通过词向量可视化，你能发现哪些有趣的语义关系？
4. 语义关系之中，例如，「man」和「programmer」可能距离更近，而「woman」和「programmer」距离更远，这反映了现实世界中的性别偏见。你觉得这种偏见是如何产生的？我们应该如何应对它？
5. 你觉得机器真的理解了语言吗？还是它只是通过统计学方法捕捉了词语之间的关系？

#### 作业 2 - 作业要求

在完成作业后，你需要写一份**文档**，内容包括但不限于：

1. 对比分析图
2. 代码实现细节
3. 词向量可视化图
4. （可选）展示至少 3 个句子的 Attention 热力图，并且分析模型的可解释性。如果模型判断错了，通过热力图能够看出原因吗？

以及你遇到的问题，等等。

### 作业 3

详见 CV 方向的作业 3。
